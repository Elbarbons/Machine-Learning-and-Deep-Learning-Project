{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqWAiw5uOy7y97gd6Qy+L2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o32b28KlHSmr"},"outputs":[],"source":["from pytorch_metric_learning import losses, miners\n","from pytorch_metric_learning.distances import CosineSimilarity, DotProductSimilarity\n","from pytorch_metric_learning.miners import BaseMiner\n","from pytorch_metric_learning.utils import loss_and_miner_utils as lmu\n","from pytorch_metric_learning.utils import common_functions as c_f\n","import torch\n","\n","import numpy as np\n","from tqdm import tqdm\n","import time\n","\n","##########\n","#This file contains the support methods to retrieve loss functions and miners from a list of notable ones\n","#Is always possible to use different loss funtions or miners by manually providing them to the net model\n","##########\n","\n","\n","#Loss functions\n","def get_loss(loss_name):\n","    if loss_name == 'SupConLoss': return losses.SupConLoss(temperature=0.07)\n","    if loss_name == 'CircleLoss': return losses.CircleLoss(m=0.4, gamma=80) #these are params for image retrieval\n","    if loss_name == 'MultiSimilarityLoss': return losses.MultiSimilarityLoss(alpha=1.0, beta=50, base=0.0, distance=DotProductSimilarity())\n","    if loss_name == 'ContrastiveLoss': return losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n","    if loss_name == 'Lifted': return losses.GeneralizedLiftedStructureLoss(neg_margin=0, pos_margin=1, distance=DotProductSimilarity())\n","    if loss_name == 'FastAPLoss': return losses.FastAPLoss(num_bins=10)\n","    if loss_name == 'NTXentLoss': return losses.NTXentLoss(temperature=0.07) #The MoCo paper uses 0.07, while SimCLR uses 0.5.\n","    if loss_name == 'TripletMarginLoss': return losses.TripletMarginLoss(margin=0.1, swap=False, smooth_loss=False, triplets_per_anchor='all') #or an int, for example 100\n","    if loss_name == 'CentroidTripletLoss': return losses.CentroidTripletLoss(margin=0.05,\n","                                                                            swap=False,\n","                                                                            smooth_loss=False,\n","                                                                            triplets_per_anchor=\"all\",)\n","    raise NotImplementedError(f'Sorry, <{loss_name}> loss function is not implemented!')\n","\n","#Miners\n","def get_miner(miner_name, margin=0.1):\n","    if miner_name == 'TripletMarginMiner' : return miners.TripletMarginMiner(margin=margin, type_of_triplets=\"semihard\") # all, hard, semihard, easy\n","    if miner_name == 'MultiSimilarityMiner' : return miners.MultiSimilarityMiner(epsilon=margin, distance=CosineSimilarity())\n","    if miner_name == 'PairMarginMiner' : return miners.PairMarginMiner(pos_margin=0.7, neg_margin=0.3, distance=DotProductSimilarity())\n","\n","    if miner_name == 'NewTripletMarginMiner': return NewTripletMarginMiner(margin=margin, type_of_triplets=\"semihard\")\n","    if miner_name == 'NewMultiSimilarityMiner': return NewMultiSimilarityMiner(epsilon=margin, distance=CosineSimilarity())\n","    if miner_name == 'NewPairMarginMiner' : return NewPairMarginMiner(pos_margin=0.7, neg_margin=0.3, distance=DotProductSimilarity())\n","\n","    return None\n","\n","#New implementation of the MultiSimilarity Miner\n","class NewMultiSimilarityMiner(BaseMiner):\n","    def __init__(self, epsilon=0.1, **kwargs):\n","        super().__init__(**kwargs)\n","        self.epsilon = epsilon\n","        self.add_to_recordable_attributes(name=\"epsilon\", is_stat=False)\n","\n","        #Please notice that the following parameter is hardcoded and must be updated every time the\n","        #miner is used in a new environment\n","        PATH_TO_DICT = '/content/drive/MyDrive/Colab_Notebooks/Dataframes/positives_classes.npy'\n","        self.additional_positive_instances = np.load(PATH_TO_DICT, allow_pickle='TRUE').item()\n","\n","    def mine(self, embeddings, labels, ref_emb, ref_labels):\n","        mat = self.distance(embeddings, ref_emb)\n","        a1, p, a2, n = self.get_all_pairs_indices(labels, ref_labels)\n","\n","        if len(a1) == 0 or len(a2) == 0:\n","            empty = torch.tensor([], device=labels.device, dtype=torch.long)\n","            return empty.clone(), empty.clone(), empty.clone(), empty.clone()\n","\n","        mat_neg_sorting = mat\n","        mat_pos_sorting = mat.clone()\n","\n","        dtype = mat.dtype\n","        pos_ignore = (\n","            c_f.pos_inf(dtype) if self.distance.is_inverted else c_f.neg_inf(dtype)\n","        )\n","        neg_ignore = (\n","            c_f.neg_inf(dtype) if self.distance.is_inverted else c_f.pos_inf(dtype)\n","        )\n","\n","        mat_pos_sorting[a2, n] = pos_ignore\n","        mat_neg_sorting[a1, p] = neg_ignore\n","        if embeddings is ref_emb:\n","            mat_pos_sorting.fill_diagonal_(pos_ignore)\n","            mat_neg_sorting.fill_diagonal_(neg_ignore)\n","\n","        pos_sorted, pos_sorted_idx = torch.sort(mat_pos_sorting, dim=1)\n","        neg_sorted, neg_sorted_idx = torch.sort(mat_neg_sorting, dim=1)\n","\n","        if self.distance.is_inverted:\n","            hard_pos_idx = torch.where(\n","                pos_sorted - self.epsilon < neg_sorted[:, -1].unsqueeze(1)\n","            )\n","            hard_neg_idx = torch.where(\n","                neg_sorted + self.epsilon > pos_sorted[:, 0].unsqueeze(1)\n","            )\n","        else:\n","            hard_pos_idx = torch.where(\n","                pos_sorted + self.epsilon > neg_sorted[:, 0].unsqueeze(1)\n","            )\n","            hard_neg_idx = torch.where(\n","                neg_sorted - self.epsilon < pos_sorted[:, -1].unsqueeze(1)\n","            )\n","\n","        a1 = hard_pos_idx[0]\n","        p = pos_sorted_idx[a1, hard_pos_idx[1]]\n","        a2 = hard_neg_idx[0]\n","        n = neg_sorted_idx[a2, hard_neg_idx[1]]\n","\n","        return a1, p, a2, n\n","\n","    def get_default_distance(self):\n","        return CosineSimilarity()\n","\n","    def get_all_pairs_indices(self, labels, ref_labels=None):\n","          \"\"\"\n","          Given a tensor of labels, this will return 4 tensors.\n","          The first 2 tensors are the indices which form all positive pairs\n","          The second 2 tensors are the indices which form all negative pairs\n","          \"\"\"\n","          matches, diffs = self.get_matches_and_diffs(labels, ref_labels)\n","          a1_idx, p_idx = torch.where(matches)\n","          a2_idx, n_idx = torch.where(diffs)\n","          return a1_idx, p_idx, a2_idx, n_idx\n","\n","    def output_assertion(self, output):\n","        \"\"\"\n","        Args:\n","            output: the output of self.mine\n","        This asserts that the mining function is outputting\n","        properly formatted indices. The default is to require a tuple representing\n","        a,p,n indices or a1,p,a2,n indices within a batch of embeddings.\n","        For example, a tuple of (anchors, positives, negatives) will be\n","        (torch.tensor, torch.tensor, torch.tensor)\n","        \"\"\"\n","        if len(output) == 3:\n","            self.num_triplets = len(output[0])\n","            assert self.num_triplets == len(output[1]) == len(output[2])\n","        elif len(output) == 4:\n","            self.num_pos_pairs = len(output[0])\n","            self.num_neg_pairs = len(output[2])\n","            assert self.num_pos_pairs == len(output[1])\n","            assert self.num_neg_pairs == len(output[3])\n","        else:\n","            raise TypeError\n","\n","    def get_matches_and_diffs(self, labels, ref_labels=None):\n","\n","        if ref_labels is None:\n","            ref_labels = labels\n","\n","        labels1 = labels.unsqueeze(1)\n","        labels2 = ref_labels.unsqueeze(0)\n","        matches = (labels1 == labels2).byte()\n","        diffs = matches ^ 1\n","\n","        labels_list = labels.tolist()\n","        ref_labels_list = ref_labels.tolist()\n","        ref_labels_indexes = {}\n","\n","        for i, element in enumerate(ref_labels_list):\n","            if element not in ref_labels_indexes:\n","                ref_labels_indexes[element] = []\n","            ref_labels_indexes[element].append(i)\n","\n","        modification_done = 0\n","\n","        for i, label in enumerate(labels_list):\n","            if label in self.additional_positive_instances:\n","                positives = self.additional_positive_instances[label]\n","                for pos in positives:\n","                    if pos in ref_labels_indexes:\n","                        for index in ref_labels_indexes[pos]:\n","                            modification_done+=1\n","                            matches[i, index] = 1\n","                            matches[index, i] = 1\n","\n","        if ref_labels is labels:\n","            matches.fill_diagonal_(0)\n","\n","        return matches, diffs\n","\n","#New implementation of the TripletMargin Miner\n","class NewTripletMarginMiner(BaseMiner):\n","    \"\"\"\n","    Returns triplets that violate the margin\n","    Args:\n","        margin\n","        type_of_triplets: options are \"all\", \"hard\", or \"semihard\".\n","                \"all\" means all triplets that violate the margin\n","                \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive\n","                \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive\n","            \"easy\" is all triplets that are not in \"all\"\n","    \"\"\"\n","\n","    def __init__(self, margin=0.2, type_of_triplets=\"all\", **kwargs):\n","        super().__init__(**kwargs)\n","        self.margin = margin\n","        self.type_of_triplets = type_of_triplets\n","        self.add_to_recordable_attributes(list_of_names=[\"margin\"], is_stat=False)\n","        self.add_to_recordable_attributes(\n","            list_of_names=[\"avg_triplet_margin\", \"pos_pair_dist\", \"neg_pair_dist\"],\n","            is_stat=True,\n","        )\n","\n","        #Please notice that the following parameter is hardcoded and must be updated every time the\n","        #miner is used in a new environment\n","        PATH_TO_DICT = '/content/drive/MyDrive/Colab_Notebooks/Dataframes/positives_classes.npy'\n","        self.additional_positive_instances = np.load(PATH_TO_DICT, allow_pickle='TRUE').item()\n","\n","    def mine(self, embeddings, labels, ref_emb, ref_labels):\n","        anchor_idx, positive_idx, negative_idx = self.get_all_triplets_indices(\n","            labels, ref_labels\n","        )\n","        mat = self.distance(embeddings, ref_emb)\n","        ap_dist = mat[anchor_idx, positive_idx]\n","        an_dist = mat[anchor_idx, negative_idx]\n","        triplet_margin = (\n","            ap_dist - an_dist if self.distance.is_inverted else an_dist - ap_dist\n","        )\n","\n","        self.set_stats(ap_dist, an_dist, triplet_margin)\n","\n","        if self.type_of_triplets == \"easy\":\n","            threshold_condition = triplet_margin > self.margin\n","        else:\n","            threshold_condition = triplet_margin <= self.margin\n","            if self.type_of_triplets == \"hard\":\n","                threshold_condition &= triplet_margin <= 0\n","            elif self.type_of_triplets == \"semihard\":\n","                threshold_condition &= triplet_margin > 0\n","\n","        return (\n","            anchor_idx[threshold_condition],\n","            positive_idx[threshold_condition],\n","            negative_idx[threshold_condition],\n","        )\n","\n","    def get_all_triplets_indices(self, labels, ref_labels=None):\n","          all_matches, all_diffs = self.get_matches_and_diffs(labels, ref_labels)\n","\n","          if (\n","              all_matches.shape[0] * all_matches.shape[1] * all_matches.shape[1]\n","              < torch.iinfo(torch.int32).max\n","          ):\n","              # torch.nonzero is not supported for tensors with more than INT_MAX elements\n","              return self.get_all_triplets_indices_vectorized_method(all_matches, all_diffs)\n","\n","          return self.get_all_triplets_indices_loop_method(labels, all_matches, all_diffs)\n","\n","    def set_stats(self, ap_dist, an_dist, triplet_margin):\n","        if self.collect_stats:\n","            with torch.no_grad():\n","                self.pos_pair_dist = torch.mean(ap_dist).item()\n","                self.neg_pair_dist = torch.mean(an_dist).item()\n","                self.avg_triplet_margin = torch.mean(triplet_margin).item()\n","\n","    def get_matches_and_diffs(self, labels, ref_labels=None):\n","\n","        if ref_labels is None:\n","            ref_labels = labels\n","\n","        labels1 = labels.unsqueeze(1)\n","        labels2 = ref_labels.unsqueeze(0)\n","        matches = (labels1 == labels2).byte()\n","        diffs = matches ^ 1\n","\n","        labels_list = labels.tolist()\n","        ref_labels_list = ref_labels.tolist()\n","        ref_labels_indexes = {}\n","\n","        for i, element in enumerate(ref_labels_list):\n","            if element not in ref_labels_indexes:\n","                ref_labels_indexes[element] = []\n","            ref_labels_indexes[element].append(i)\n","\n","        modification_done = 0\n","\n","        for i, label in enumerate(labels_list):\n","            if label in self.additional_positive_instances:\n","                positives = self.additional_positive_instances[label]\n","                for pos in positives:\n","                    if pos in ref_labels_indexes:\n","                        for index in ref_labels_indexes[pos]:\n","                            modification_done+=1\n","                            matches[i, index] = 1\n","                            matches[index, i] = 1\n","\n","        if ref_labels is labels:\n","            matches.fill_diagonal_(0)\n","\n","        return matches, diffs\n","\n","    def get_all_triplets_indices_vectorized_method(self, all_matches, all_diffs):\n","        triplets = all_matches.unsqueeze(2) * all_diffs.unsqueeze(1)\n","        return torch.where(triplets)\n","\n","\n","    def get_all_triplets_indices_loop_method(self, labels, all_matches, all_diffs):\n","        all_matches, all_diffs = all_matches.bool(), all_diffs.bool()\n","\n","        # Find anchors with at least a positive and a negative\n","        indices = torch.arange(0, len(labels), device=labels.device)\n","        indices = indices[all_matches.any(dim=1) & all_diffs.any(dim=1)]\n","\n","        # No triplets found\n","        if len(indices) == 0:\n","            return (\n","                torch.tensor([], device=labels.device, dtype=labels.dtype),\n","                torch.tensor([], device=labels.device, dtype=labels.dtype),\n","                torch.tensor([], device=labels.device, dtype=labels.dtype),\n","            )\n","\n","        # Compute all triplets\n","        anchors = []\n","        positives = []\n","        negatives = []\n","        for i in indices:\n","            matches = all_matches[i].nonzero(as_tuple=False).squeeze(1)\n","            diffs = all_diffs[i].nonzero(as_tuple=False).squeeze(1)\n","            nd = len(diffs)\n","            nm = len(matches)\n","            matches = matches.repeat_interleave(nd)\n","            diffs = diffs.repeat(nm)\n","            anchors.append(\n","                torch.full((len(matches),), i, dtype=labels.dtype, device=labels.device)\n","            )\n","            positives.append(matches)\n","            negatives.append(diffs)\n","        return torch.cat(anchors), torch.cat(positives), torch.cat(negatives)\n","\n","    def output_assertion(self, output):\n","        \"\"\"\n","        Args:\n","            output: the output of self.mine\n","        This asserts that the mining function is outputting\n","        properly formatted indices. The default is to require a tuple representing\n","        a,p,n indices or a1,p,a2,n indices within a batch of embeddings.\n","        For example, a tuple of (anchors, positives, negatives) will be\n","        (torch.tensor, torch.tensor, torch.tensor)\n","        \"\"\"\n","        if len(output) == 3:\n","            self.num_triplets = len(output[0])\n","            assert self.num_triplets == len(output[1]) == len(output[2])\n","        elif len(output) == 4:\n","            self.num_pos_pairs = len(output[0])\n","            self.num_neg_pairs = len(output[2])\n","            assert self.num_pos_pairs == len(output[1])\n","            assert self.num_neg_pairs == len(output[3])\n","        else:\n","            raise TypeError\n","\n","#New implementation of the PairMargin Miner\n","class NewPairMarginMiner(BaseMiner):\n","    \"\"\"\n","    Returns positive pairs that have distance greater than a margin and negative\n","    pairs that have distance less than a margin\n","    \"\"\"\n","\n","    def __init__(self, pos_margin=0.2, neg_margin=0.8, **kwargs):\n","        super().__init__(**kwargs)\n","        self.pos_margin = pos_margin\n","        self.neg_margin = neg_margin\n","        self.add_to_recordable_attributes(\n","            list_of_names=[\"pos_margin\", \"neg_margin\"], is_stat=False\n","        )\n","        self.add_to_recordable_attributes(\n","            list_of_names=[\"pos_pair_dist\", \"neg_pair_dist\"], is_stat=True\n","        )\n","\n","        #Please notice that the following parameter is hardcoded and must be updated every time the\n","        #miner is used in a new environment\n","        PATH_TO_DICT = '/content/drive/MyDrive/Colab_Notebooks/Dataframes/positives_classes.npy'\n","        self.additional_positive_instances = np.load(PATH_TO_DICT, allow_pickle='TRUE').item()\n","\n","    def mine(self, embeddings, labels, ref_emb, ref_labels):\n","        mat = self.distance(embeddings, ref_emb)\n","        a1, p, a2, n = self.get_all_pairs_indices(labels, ref_labels)\n","        pos_pair = mat[a1, p]\n","        neg_pair = mat[a2, n]\n","        self.set_stats(pos_pair, neg_pair)\n","        pos_mask = (\n","            pos_pair < self.pos_margin\n","            if self.distance.is_inverted\n","            else pos_pair > self.pos_margin\n","        )\n","        neg_mask = (\n","            neg_pair > self.neg_margin\n","            if self.distance.is_inverted\n","            else neg_pair < self.neg_margin\n","        )\n","        return a1[pos_mask], p[pos_mask], a2[neg_mask], n[neg_mask]\n","\n","    def set_stats(self, pos_pair, neg_pair):\n","        if self.collect_stats:\n","            with torch.no_grad():\n","                self.pos_pair_dist = (\n","                    torch.mean(pos_pair).item() if len(pos_pair) > 0 else 0\n","                )\n","                self.neg_pair_dist = (\n","                    torch.mean(neg_pair).item() if len(neg_pair) > 0 else 0\n","                )\n","\n","    def get_all_pairs_indices(self, labels, ref_labels=None):\n","          \"\"\"\n","          Given a tensor of labels, this will return 4 tensors.\n","          The first 2 tensors are the indices which form all positive pairs\n","          The second 2 tensors are the indices which form all negative pairs\n","          \"\"\"\n","          matches, diffs = self.get_matches_and_diffs(labels, ref_labels)\n","          a1_idx, p_idx = torch.where(matches)\n","          a2_idx, n_idx = torch.where(diffs)\n","          return a1_idx, p_idx, a2_idx, n_idx\n","\n","    def get_matches_and_diffs(self, labels, ref_labels=None):\n","\n","        if ref_labels is None:\n","            ref_labels = labels\n","\n","        labels1 = labels.unsqueeze(1)\n","        labels2 = ref_labels.unsqueeze(0)\n","        matches = (labels1 == labels2).byte()\n","        diffs = matches ^ 1\n","\n","        labels_list = labels.tolist()\n","        ref_labels_list = ref_labels.tolist()\n","        ref_labels_indexes = {}\n","\n","        for i, element in enumerate(ref_labels_list):\n","            if element not in ref_labels_indexes:\n","                ref_labels_indexes[element] = []\n","            ref_labels_indexes[element].append(i)\n","\n","        modification_done = 0\n","\n","        for i, label in enumerate(labels_list):\n","            if label in self.additional_positive_instances:\n","                positives = self.additional_positive_instances[label]\n","                for pos in positives:\n","                    if pos in ref_labels_indexes:\n","                        for index in ref_labels_indexes[pos]:\n","                            modification_done+=1\n","                            matches[i, index] = 1\n","                            matches[index, i] = 1\n","\n","        if ref_labels is labels:\n","            matches.fill_diagonal_(0)\n","\n","        return matches, diffs\n","\n","    def output_assertion(self, output):\n","        \"\"\"\n","        Args:\n","            output: the output of self.mine\n","        This asserts that the mining function is outputting\n","        properly formatted indices. The default is to require a tuple representing\n","        a,p,n indices or a1,p,a2,n indices within a batch of embeddings.\n","        For example, a tuple of (anchors, positives, negatives) will be\n","        (torch.tensor, torch.tensor, torch.tensor)\n","        \"\"\"\n","        if len(output) == 3:\n","            self.num_triplets = len(output[0])\n","            assert self.num_triplets == len(output[1]) == len(output[2])\n","        elif len(output) == 4:\n","            self.num_pos_pairs = len(output[0])\n","            self.num_neg_pairs = len(output[2])\n","            assert self.num_pos_pairs == len(output[1])\n","            assert self.num_neg_pairs == len(output[3])\n","        else:\n","            raise TypeError"]}]}