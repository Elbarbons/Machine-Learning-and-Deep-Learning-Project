{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSYK99OqZmUrzuGrgEdiX4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1MnromEvIbNM"},"outputs":[],"source":["import pytorch_lightning as pl\n","from torch.utils.data.dataloader import DataLoader\n","from torchvision import transforms as T\n","\n","from GSVCitiesDataset import GSVCitiesDataset\n","from SanFranciscoDataset import SanFranciscoValidationDataset, SanFranciscoTestDataset\n","from TokyoDataset import TokyoTestDataset\n","\n","\n","from prettytable import PrettyTable\n","\n","IMAGENET_MEAN_STD = {'mean': [0.485, 0.456, 0.406],\n","                     'std': [0.229, 0.224, 0.225]}\n","\n","VIT_MEAN_STD = {'mean': [0.5, 0.5, 0.5],\n","                'std': [0.5, 0.5, 0.5]}\n","\n","TRAIN_CITIES = [\n","    'Bangkok',\n","    'Buenosaires',\n","    'Losangeles',\n","    'Mexicocity',\n","    'Osl', # refers to Oslo\n","    'Rome',\n","    'Barcelona',\n","    'Chicago',\n","    'Madrid',\n","    'Miami',\n","    'Phoenix',\n","    'Trt', # refers to Toronto\n","    'Boston',\n","    'Lisbon',\n","    'Medellin',\n","    'Minneapolis',\n","    'Prg', # refers to Prague\n","    'Washingtondc',\n","    'Brussels',\n","    'London',\n","    'Melbourne',\n","    'Osaka',\n","    'Prs', # refers to Paris\n","]\n","\n","\n","class GSVCitiesDataModule(pl.LightningDataModule):\n","    def __init__(self,\n","                 batch_size=32,\n","                 img_per_place=4,\n","                 min_img_per_place=4,\n","                 shuffle_all=False,\n","                 image_size=(480, 640),\n","                 num_workers=4,\n","                 show_data_stats=True,\n","                 cities=TRAIN_CITIES,\n","                 mean_std=IMAGENET_MEAN_STD,\n","                 batch_sampler=None,\n","                 random_sample_from_each_place=True,\n","                 val_set_names=['SanFrancisco_xs'],\n","                 test_set_names=['SanFrancisco_xs', 'Tokyo_xs']\n","                 ):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.img_per_place = img_per_place\n","        self.min_img_per_place = min_img_per_place\n","        self.shuffle_all = shuffle_all\n","        self.image_size = image_size\n","        self.num_workers = num_workers\n","        self.batch_sampler = batch_sampler\n","        self.show_data_stats = show_data_stats\n","        self.cities = cities\n","        self.mean_dataset = mean_std['mean']\n","        self.std_dataset = mean_std['std']\n","        self.random_sample_from_each_place = random_sample_from_each_place\n","        self.val_set_names = val_set_names\n","        self.test_set_names = test_set_names\n","        self.save_hyperparameters() # save hyperparameter with Pytorch Lightening\n","\n","        self.train_transform = T.Compose([\n","            T.Resize(image_size, interpolation=T.InterpolationMode.BILINEAR),\n","            T.RandAugment(num_ops=3, interpolation=T.InterpolationMode.BILINEAR),\n","            T.ToTensor(),\n","            T.Normalize(mean=self.mean_dataset, std=self.std_dataset),\n","        ])\n","\n","        self.valid_transform = T.Compose([\n","            T.Resize(image_size, interpolation=T.InterpolationMode.BILINEAR),\n","            T.ToTensor(),\n","            T.Normalize(mean=self.mean_dataset, std=self.std_dataset)])\n","\n","        self.train_loader_config = {\n","            'batch_size': self.batch_size,\n","            'num_workers': self.num_workers,\n","            'drop_last': False,\n","            'pin_memory': True,\n","            'shuffle': self.shuffle_all}\n","\n","        self.valid_loader_config = {\n","            'batch_size': self.batch_size,\n","            'num_workers': self.num_workers//2,\n","            'drop_last': False,\n","            'pin_memory': True,\n","            'shuffle': False}\n","\n","    def setup(self, stage):\n","        if stage == 'fit':\n","            # load train dataloader with reload routine\n","            self.reload()\n","\n","            # load validation sets (pitts_val, msls_val, ...etc)\n","            self.val_datasets = []\n","            for valid_set_name in self.val_set_names:\n","                if 'sanfrancisco' in valid_set_name.lower():\n","                    self.val_datasets.append(SanFranciscoValidationDataset(\n","                        input_transform=self.valid_transform))\n","                else:\n","                    print(\n","                        f'Validation set {valid_set_name} does not exist or has not been implemented yet')\n","                    raise NotImplementedError\n","\n","            if self.show_data_stats:\n","                self.print_stats(stage)\n","\n","\n","        if stage == \"test\":\n","            self.test_datasets = []\n","            for valid_set_name in self.test_set_names:\n","                if 'sanfrancisco_xs' in valid_set_name.lower():\n","                    self.test_datasets.append(SanFranciscoTestDataset(input_transform=self.valid_transform))\n","\n","                elif 'tokyo_xs' in valid_set_name.lower():\n","                    self.test_datasets.append(TokyoTestDataset(input_transform=self.valid_transform))\n","\n","                else:\n","                    print(f'Test set {valid_set_name} does not exist or has not been implemented yet')\n","                    raise NotImplementedError\n","\n","            if self.show_data_stats:\n","                self.print_stats(stage)\n","\n","    def reload(self):\n","        self.train_dataset = GSVCitiesDataset(\n","            cities=self.cities,\n","            img_per_place=self.img_per_place,\n","            min_img_per_place=self.min_img_per_place,\n","            random_sample_from_each_place=self.random_sample_from_each_place,\n","            transform=self.train_transform)\n","\n","    def train_dataloader(self):\n","        self.reload()\n","        return DataLoader(dataset=self.train_dataset, **self.train_loader_config)\n","\n","    def val_dataloader(self):\n","        val_dataloaders = []\n","        for val_dataset in self.val_datasets:\n","            val_dataloaders.append(DataLoader(\n","                dataset=val_dataset, **self.valid_loader_config))\n","        return val_dataloaders\n","\n","    def test_dataloader(self):\n","        test_dataloaders = []\n","        for test_dataset in self.test_datasets:\n","            test_dataloaders.append(DataLoader(\n","                dataset=test_dataset, **self.valid_loader_config))\n","        return test_dataloaders\n","\n","    def print_stats(self, stage):\n","        if stage == 'fit':\n","            print()  # print a new line\n","            table = PrettyTable()\n","            table.field_names = ['Data', 'Value']\n","            table.align['Data'] = \"l\"\n","            table.align['Value'] = \"l\"\n","            table.header = False\n","            table.add_row([\"# of cities\", f\"{len(TRAIN_CITIES)}\"])\n","            table.add_row([\"# of places\", f'{self.train_dataset.__len__()}'])\n","            table.add_row([\"# of images\", f'{self.train_dataset.total_nb_images}'])\n","            print(table.get_string(title=\"Training Dataset\"))\n","            print()\n","\n","            table = PrettyTable()\n","            table.field_names = ['Data', 'Value']\n","            table.align['Data'] = \"l\"\n","            table.align['Value'] = \"l\"\n","            table.header = False\n","            for i, val_set_name in enumerate(self.val_set_names):\n","                table.add_row([f\"Validation set {i+1}\", f\"{val_set_name}\"])\n","            # table.add_row([\"# of places\", f'{self.train_dataset.__len__()}'])\n","            print(table.get_string(title=\"Validation Datasets\"))\n","            print()\n","\n","            table = PrettyTable()\n","            table.field_names = ['Data', 'Value']\n","            table.align['Data'] = \"l\"\n","            table.align['Value'] = \"l\"\n","            table.header = False\n","            table.add_row(\n","                [\"Batch size (PxK)\", f\"{self.batch_size}x{self.img_per_place}\"])\n","            table.add_row(\n","                [\"# of iterations\", f\"{self.train_dataset.__len__()//self.batch_size}\"])\n","            table.add_row([\"Image size\", f\"{self.image_size}\"])\n","            print(table.get_string(title=\"Training config\"))\n","\n","        if stage == 'test':\n","            table = PrettyTable()\n","            table.field_names = ['Data', 'Value']\n","            table.align['Data'] = \"l\"\n","            table.align['Value'] = \"l\"\n","            table.header = False\n","            for i, test_set_name in enumerate(self.test_set_names):\n","                table.add_row([f\"Test set {i+1}\", f\"{test_set_name}\"])\n","            print(table.get_string(title=\"Test Datasets\"))\n","            print()"]}]}