{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2qJLcOAI6FyBaBcLmAGPr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HQuT8_XGIhpe"},"outputs":[],"source":["import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","import torchvision.transforms as T\n","\n","default_transform = T.Compose([\n","    T.ToTensor(),\n","    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","\n","#PLEASE NOTICE\n","#the following values are hardcoded and they must be edited before running\n","#the program in any new environment\n","DATASET_PATH = '/content/Datasets/'\n","DATAFRAMES_PATH = '/content/drive/MyDrive/Colab_Notebooks/Dataframes/'\n","\n","if not Path(DATASET_PATH).exists():\n","    raise FileNotFoundError(\n","        'DATASET_PATH is hardcoded, please adjust to point to gsv_cities')\n","\n","if not Path(DATAFRAMES_PATH).exists():\n","    raise FileNotFoundError(\n","        'DATAFRAMES_PATH is hardcoded, please adjust to point to gsv_cities')\n","\n","class GSVCitiesDataset(Dataset):\n","    def __init__(self,\n","                 cities=['London', 'Boston'],\n","                 img_per_place=4,\n","                 min_img_per_place=4,\n","                 random_sample_from_each_place=True,\n","                 transform=default_transform,\n","                 dataframes_path=DATAFRAMES_PATH,\n","                 dataset_path=DATASET_PATH\n","                 ):\n","        super(GSVCitiesDataset, self).__init__()\n","        self.dataframes_path = dataframes_path\n","        self.dataset_path = dataset_path\n","        self.cities = cities\n","\n","        assert img_per_place <= min_img_per_place, \\\n","            f\"img_per_place should be less than {min_img_per_place}\"\n","        self.img_per_place = img_per_place\n","        self.min_img_per_place = min_img_per_place\n","        self.random_sample_from_each_place = random_sample_from_each_place\n","        self.transform = transform\n","\n","        # generate the dataframe contraining images metadata\n","        self.dataframe = self.__getdataframes()\n","\n","        # get all unique place ids\n","        self.places_ids = pd.unique(self.dataframe.index)\n","        self.total_nb_images = len(self.dataframe)\n","\n","    def __getdataframes(self):\n","        '''\n","            Return one dataframe containing\n","            all info about the images from all cities\n","\n","            This requieres DataFrame files to be in a folder\n","            named Dataframes, containing a DataFrame\n","            for each city in self.cities\n","        '''\n","        # read the first city dataframe\n","        df = pd.read_csv(self.dataframes_path+f'{self.cities[0]}.csv')\n","        df = df.sample(frac=1)  # shuffle the city dataframe\n","\n","\n","        # append other cities one by one\n","        for i in range(1, len(self.cities)):\n","            tmp_df = pd.read_csv(\n","                self.dataframes_path+f'{self.cities[i]}.csv')\n","\n","            # Now we add a prefix to place_id, so that we\n","            # don't confuse, say, place number 13 of NewYork\n","            # with place number 13 of London ==> (0000013 and 0500013)\n","            # We suppose that there is no city with more than\n","            # 99999 images and there won't be more than 99 cities\n","            # TODO: rename the dataset and hardcode these prefixes\n","            prefix = i\n","            tmp_df['Id'] = tmp_df['Id'] + (prefix * 10**5)\n","            tmp_df = tmp_df.sample(frac=1)  # shuffle the city dataframe\n","\n","            df = pd.concat([df, tmp_df], ignore_index=True)\n","\n","        # keep only places depicted by at least min_img_per_place images\n","        res = df[df.groupby('Id')['Id'].transform(\n","            'size') >= self.min_img_per_place]\n","        return res.set_index('Id')\n","\n","    def __getitem__(self, index):\n","        place_id = self.places_ids[index]\n","\n","        # get the place in form of a dataframe (each row corresponds to one image)\n","        place = self.dataframe.loc[place_id]\n","\n","        # sample K images (rows) from this place\n","        # we can either sort and take the most recent k images\n","        # or randomly sample them\n","        place = place.sample(n=self.img_per_place)\n","\n","        imgs = []\n","        for i, row in place.iterrows():\n","            img_path = self.dataset_path + row[\"Path\"]\n","            img = self.image_loader(img_path)\n","\n","            if self.transform is not None:\n","                img = self.transform(img)\n","\n","            imgs.append(img)\n","\n","        # NOTE: contrary to image classification where __getitem__ returns only one image\n","        # in GSVCities, we return a place, which is a Tesor of K images (K=self.img_per_place)\n","        # this will return a Tensor of shape [K, channels, height, width]. This needs to be taken into account\n","        # in the Dataloader (which will yield batches of shape [BS, K, channels, height, width])\n","        return torch.stack(imgs), torch.tensor(place_id).repeat(self.img_per_place)\n","\n","    def __len__(self):\n","        '''Denotes the total number of places (not images)'''\n","        return len(self.places_ids)\n","\n","    @staticmethod\n","    def image_loader(path):\n","        return Image.open(path).convert('RGB')"]}]}